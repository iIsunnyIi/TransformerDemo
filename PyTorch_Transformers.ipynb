{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch-Transformers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOS8C4/OLrI5+fjFhuGGRYi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iIsunnyIi/TransformerDemo/blob/main/PyTorch_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skyM97Jh8SoS"
      },
      "source": [
        "##Predicting the next word using GPT-**2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWBvTFVK7Q0u",
        "outputId": "a9d978f6-ee04-4ae9-c889-d84b0c13353f"
      },
      "source": [
        "!pip install pytorch-transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 11.4MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/38/686d114fadf6403b4e8cd782776eb5b0dff18d1983c5e4567363b130fac9/boto3-1.16.37-py2.py3-none-any.whl (130kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 31.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.18.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 32.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 51.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.41.1)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.7MB/s \n",
            "\u001b[?25hCollecting botocore<1.20.0,>=1.19.37\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/cc/a8309150620477ff1444f7f8fc095db0decad6267bb298e8f8721b9c0d20/botocore-1.19.37-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 36.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.17.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.37->boto3->pytorch-transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=fe2e98b2409139a2d31ea635d530638269582285af57e92609780b1f3f2f02de\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: botocore 1.19.37 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, sentencepiece, sacremoses, pytorch-transformers\n",
            "Successfully installed boto3-1.16.37 botocore-1.19.37 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJwelZrg8Rnb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lidja4yG7zSj",
        "outputId": "e59cf17f-4df3-4ee2-e59c-ea6251fccbec"
      },
      "source": [
        "# Import required libraries\r\n",
        "import torch\r\n",
        "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel\r\n",
        "\r\n",
        "# Load pre-trained model tokenizer (vocabulary)\r\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\r\n",
        "\r\n",
        "# Encode a text inputs\r\n",
        "text = \"What is the fastest car in the\"\r\n",
        "indexed_tokens = tokenizer.encode(text)\r\n",
        "\r\n",
        "# Convert indexed tokens in a PyTorch tensor\r\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\r\n",
        "\r\n",
        "# Load pre-trained model (weights)\r\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\r\n",
        "\r\n",
        "# Set the model in evaluation mode to deactivate the DropOut modules\r\n",
        "model.eval()\r\n",
        "\r\n",
        "# If you have a GPU, put everything on cuda\r\n",
        "tokens_tensor = tokens_tensor.to('cuda')\r\n",
        "model.to('cuda')\r\n",
        "\r\n",
        "# Predict all tokens\r\n",
        "with torch.no_grad():\r\n",
        "    outputs = model(tokens_tensor)\r\n",
        "    predictions = outputs[0]\r\n",
        "\r\n",
        "# Get the predicted next sub-word\r\n",
        "predicted_index = torch.argmax(predictions[0, -1, :]).item()\r\n",
        "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\r\n",
        "\r\n",
        "# Print the predicted word\r\n",
        "print(predicted_text)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1042301/1042301 [00:00<00:00, 32269623.57B/s]\n",
            "100%|██████████| 456318/456318 [00:00<00:00, 18976921.68B/s]\n",
            "100%|██████████| 665/665 [00:00<00:00, 176476.57B/s]\n",
            "100%|██████████| 548118077/548118077 [00:10<00:00, 53297919.75B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " What is the fastest car in the world\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUqOWUe_8ZNZ"
      },
      "source": [
        "##Natural Language Generation using GPT-2, Transformer-XL and XLNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPACDkVn8eXO",
        "outputId": "a676f463-e2b1-4ef0-f716-df5b60a63205"
      },
      "source": [
        "!git clone https://github.com/huggingface/pytorch-transformers.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch-transformers'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 56836 (delta 0), reused 5 (delta 0), pack-reused 56829\u001b[K\n",
            "Receiving objects: 100% (56836/56836), 42.29 MiB | 27.25 MiB/s, done.\n",
            "Resolving deltas: 100% (39856/39856), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kITk5suu8QV",
        "outputId": "195972f7-7a92-4c42-9b5d-24d8a8c28994"
      },
      "source": [
        "!python pytorch-transformers/examples/run_generation.py \\\r\n",
        "    --model_type=gpt2 \\\r\n",
        "    --length=100 \\\r\n",
        "    --model_name_or_path=gpt2 \\"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file 'pytorch-transformers/examples/run_generation.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqUUT_ygvKsu",
        "outputId": "97fca83b-f6cc-4e1a-873c-cd3a2e784925"
      },
      "source": [
        "!python pytorch-transformers/examples/run_generation.py \\\r\n",
        "    --model_type=xlnet \\\r\n",
        "    --length=50 \\\r\n",
        "    --model_name_or_path=xlnet-base-cased \\"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file 'pytorch-transformers/examples/run_generation.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dKbcs43vRn7",
        "outputId": "78e408c4-c556-4a8d-a232-010a65c3f6fb"
      },
      "source": [
        "!python pytorch-transformers/examples/run_generation.py \\\r\n",
        "    --model_type=transfo-xl \\\r\n",
        "    --length=100 \\\r\n",
        "    --model_name_or_path=transfo-xl-wt103 \\"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file 'pytorch-transformers/examples/run_generation.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f38SRGdvU5Q"
      },
      "source": [
        "##Training a Masked Language Model for BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn-ofewiwGdn"
      },
      "source": [
        "So why are we doing this? The model learns the rules of the language during the training process. And we’ll soon see how effective this process is.\r\n",
        "\r\n",
        "First, let’s prepare a tokenized input from a text string using BertTokenizer:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAcs37QEvXQM",
        "outputId": "d549288c-0b27-4c8a-a1ad-9775d1addbef"
      },
      "source": [
        "import torch\r\n",
        "from pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM\r\n",
        "\r\n",
        "# Load pre-trained model tokenizer (vocabulary)\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r\n",
        "\r\n",
        "# Tokenize input\r\n",
        "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\r\n",
        "tokenized_text = tokenizer.tokenize(text)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 18047262.85B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CveffBGXwDSL"
      },
      "source": [
        "The next step would be to convert this into a sequence of integers and create PyTorch tensors of them so that we can use them directly for computation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJbuMAQbvhNq"
      },
      "source": [
        "# Mask a token that we will try to predict back with `BertForMaskedLM`\r\n",
        "masked_index = 8\r\n",
        "tokenized_text[masked_index] = '[MASK]'\r\n",
        "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\r\n",
        "\r\n",
        "# Convert token to vocabulary indices\r\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\r\n",
        "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\r\n",
        "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\r\n",
        "\r\n",
        "# Convert inputs to PyTorch tensors\r\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\r\n",
        "segments_tensors = torch.tensor([segments_ids])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8EbgRQlv_bP"
      },
      "source": [
        "Notice that we have set [MASK] at the 8th index in the sentence which is the word ‘Hensen’. This is what our model will try to predict.\r\n",
        "\r\n",
        "Now that our data is rightly pre-processed for BERT, we will create a Masked Language Model. Let’s now use BertForMaskedLM to predict a masked token:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI9qC3lmvqRD",
        "outputId": "68deb3fe-133a-4c73-8058-31d988a351b4"
      },
      "source": [
        "# Load pre-trained model (weights)\r\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\r\n",
        "model.eval()\r\n",
        "\r\n",
        "# If you have a GPU, put everything on cuda\r\n",
        "tokens_tensor = tokens_tensor.to('cuda')\r\n",
        "segments_tensors = segments_tensors.to('cuda')\r\n",
        "model.to('cuda')\r\n",
        "\r\n",
        "# Predict all tokens\r\n",
        "with torch.no_grad():\r\n",
        "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\r\n",
        "    predictions = outputs[0]\r\n",
        "\r\n",
        "# confirm we were able to predict 'henson'\r\n",
        "predicted_index = torch.argmax(predictions[0, masked_index]).item()\r\n",
        "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\r\n",
        "assert predicted_token == 'henson'\r\n",
        "print('Predicted token is:',predicted_token)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 433/433 [00:00<00:00, 282710.71B/s]\n",
            "100%|██████████| 440473133/440473133 [00:05<00:00, 78653312.70B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted token is: henson\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}